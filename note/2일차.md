<h1>강화학습 2일차 마르코프 의사결정</h1>

---

<p1>Markov Decision Process 마르코프 의사결정 프로세스</p1>

- 목표
	- 문제에 대한 에이전트의 행동의 의사 결정을 마르코프 의사결정으로 구현하기
- 마르코프 프로퍼티 Markov Property
	- 미래의 행동은 현재 상태만 영향을 준다.
-상태 전이 행렬 State Transition Matrix
	- 전이 상태
	- 같은 상태
	-확률로 표현된 행렬
- 마르코프 프로세스 Markov Process
	- 마르코프 체인 Markov Chain 이라고도 함
	- 상태가 전이될 확률을 표현
	- 각 상태의 확률의 합은 1
- 에피소드 Episodes
	- 목표 달성시까지 진행한 상태들의 배열
	- 에피소드(Episode) 단위로 샘플링(Sampling) 진행
- 마르코프 보상 프로세스 Markov Reward Process, MRP 
	- 마르코프 체인에 가치(Value) 개념 도입
	- 다음 상태로 변경시 받게 될 보상(Reward)
	- 0,1 사이의 값을 할인율(Discount Factor) 적용
- 할인율 Discount Factor
	- 감마(r) 값으로 표현
	- 일반적으로 0.9~0.99로 작성
- 리턴 Return
	- 최종 보상
	- 에피소드 단위로 리턴(Return) 값을 계산함
- 가치 함수 Value Function
	- 현재 상태에서 모든 기대 보상을 표현
	- 미래 가치가 가장 큰 의사결정을 하고 행동하는 것이 최종 목표
- 벨만 방정식 Bellman Equation
	- 동적 계획법(Dynamic Programming)에서 사용된 방정식
	- 강화학습의 시초가 된 이론
- 동적 계획법 : 노가다
- 벨만 방정식 : 가치 판단
- 행동 Action
	- 선택할 수 있는 상태의 집합
- 상태 전이 확률 행렬
	- 의도한 행동(Action)대로 진행하지 않고, 전이가 될 확률
- 정책 Policy
	- 에이전트가 어떤 행동(Action)을 취할 확률
	- 파이(Pi)로 표현됨
- 최적화된 정책
	- 가치를 최대로 하는 정책을 선택하는 것

---